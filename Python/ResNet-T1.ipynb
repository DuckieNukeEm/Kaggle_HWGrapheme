{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenetv2 for Testing - Take 2\n",
    "We are going to use a mobilnet to train a model and see how it does\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DataGenerator, Plot_Val_Test\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 #for our model\n",
    "from tensorflow.keras.applications import ResNet152V2\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in CSV Data and creating Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train is: 160585 x 7 \n",
      "Shape of Val is 40255 x 7\n"
     ]
    }
   ],
   "source": [
    "CSV = pd.read_csv('../Data/train_extra.csv')\n",
    "Counts_Per_Class = 750\n",
    "# Balancing the Classes\n",
    "\n",
    "\n",
    "# spling into train and validate\n",
    "Idx_Split = np.random.uniform(size = CSV.shape[0])\n",
    "CSV_Val = CSV[Idx_Split <= 0.2].copy().reset_index()\n",
    "CSV_Train = CSV[Idx_Split > 0.2].copy().reset_index()\n",
    "\n",
    "#rebalcing CSV_train\n",
    "if False:\n",
    "    CSV_Train['index'] = CSV_Train.index\n",
    "    Re_Index = CSV_Train.groupby('grapheme_root').index.apply(lambda x: x.sample(n=1000, replace=True))\n",
    "    CSV_Train = CSV_Train.loc[Re_Index.index.droplevel(),:]\n",
    "    CSV_Train.reset_index(inplace = True)\n",
    "    CSV_Train.drop(columns = ['index','level_0'], inplace = True)\n",
    "\n",
    "print(\"Shape of Train is: {} x {} \\nShape of Val is {} x {}\".format(CSV_Train.shape[0], \n",
    "                                                                    CSV_Train.shape[1],\n",
    "                                                                    CSV_Val.shape[0], \n",
    "                                                                    CSV_Val.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>Image_Dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "      <td>../Data/Train/Train_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "      <td>../Data/Train/Train_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "      <td>../Data/Train/Train_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "      <td>../Data/Train/Train_3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "      <td>../Data/Train/Train_4.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme  \\\n",
       "0  Train_0             15                9                    5   ক্ট্রো   \n",
       "1  Train_1            159                0                    0        হ   \n",
       "2  Train_2             22                3                    5     খ্রী   \n",
       "3  Train_3             53                2                    2     র্টি   \n",
       "4  Train_4             71                9                    5     থ্রো   \n",
       "\n",
       "                   Image_Dir  \n",
       "0  ../Data/Train/Train_0.jpg  \n",
       "1  ../Data/Train/Train_1.jpg  \n",
       "2  ../Data/Train/Train_2.jpg  \n",
       "3  ../Data/Train/Train_3.jpg  \n",
       "4  ../Data/Train/Train_4.jpg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_Train2 =  CSV[Idx_Split > 0.1].copy().reset_index()\n",
    "CSV_Val2 =  CSV[Idx_Split <= 0.1].copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encodings is 168\n",
      "Number of Encodings is 168\n",
      "Target has 168 values,\n",
      "number of batches per epoch is 2100 x 665\n"
     ]
    }
   ],
   "source": [
    "Gen_Train = DataGenerator(\n",
    "                csv_file = CSV_Train2, #CSV_Train\n",
    "                 y_var = 'grapheme_root', # 'consonant_diacritic',\n",
    "                 to_fit=True,\n",
    "                 batch_size=40, #160,\n",
    "                 dim = (90,160),\n",
    "                 channels = 1,\n",
    "                 vertical_flip = 0,\n",
    "                 horizontal_flip = 0,\n",
    "                 rotate = (0.3,180), #PRob, max roatioan\n",
    "                 shear = (0.2,1.2), # prob, max_shear\n",
    "                 shuffle=True,\n",
    "                sample_classes = 500,\n",
    "                save_model_path = '../Data/Best_Model_eva.hdf5')\n",
    "\n",
    "Gen_Val = DataGenerator(\n",
    "                csv_file = CSV_Val2, #CSV_Val\n",
    "                 y_var = 'grapheme_root', # 'consonant_diacritic',\n",
    "                 to_fit=True,\n",
    "                 batch_size=30,\n",
    "                 dim = (90,160),\n",
    "                 channels = 1,\n",
    "                 vertical_flip = 0,\n",
    "                 horizontal_flip = 0,\n",
    "                 rotate = (0,0), #Prob, max roatioan\n",
    "                 shear = (0,0), # prob, max_shear\n",
    "                 shuffle=True,\n",
    "                 sample_classes = 0)\n",
    "\n",
    "\n",
    "print(\"Target has {} values,\\nnumber of batches per epoch is {} x {}\".format(Gen_Train.y.shape[1],\n",
    "                                                                                 len(Gen_Train),\n",
    "                                                                                 len(Gen_Val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ResNet(input_shape: tuple,\n",
    "               alpha: int = 1,\n",
    "               weights: list = None,\n",
    "               dropout_per: float = 0.2,\n",
    "               target_size: int = 168,\n",
    "              learning_rate: float = 0.0002):\n",
    "    RESNET = ResNet152V2(\n",
    "                input_shape = input_shape,\n",
    "                weights=weights,\n",
    "                include_top=False)\n",
    "\n",
    "    #Making all layers in MobilenetV2 trainable!\n",
    "    #for layer in RESNET.layers:\n",
    "    #    layer.trainable = True\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(RESNET)\n",
    "    model.add(layers.GlobalMaxPooling2D())\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(1024, use_bias = False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(dropout_per))\n",
    "    \n",
    "    model.add(layers.Dense(512, use_bias = False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(dropout_per))\n",
    "    \n",
    "    model.add(layers.Dense(256, use_bias = False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Dropout(dropout_per))\n",
    "    \n",
    "    model.add(layers.Dense(target_size, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizers.Adam(lr=learning_rate),\n",
    "        #optimizer = optimizers.RMSprop(learning_rate = learning_rate )    ,\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    return model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet152v2 (Model)          (None, 3, 5, 2048)        58325376  \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              2097152   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524288    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131072    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 168)               43176     \n",
      "=================================================================\n",
      "Total params: 61,128,232\n",
      "Trainable params: 60,980,904\n",
      "Non-trainable params: 147,328\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_ResNet(input_shape = (90,160,1), dropout_per = 0.49, target_size = 168, learning_rate =  0.0001)\n",
    "#model.load_weights('../Data/Model/ResNet-20200301_batch_norm.wgt')\n",
    "#model = build_minst(input_shape = (90,160,1), dropout_per = 0.5, target_size = 168, learning_rate = 0.001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 2100 steps, validate for 665 steps\n",
      "Epoch 1/10\n",
      "2100/2100 [==============================] - 623s 297ms/step - loss: 5.3198 - accuracy: 0.0076 - val_loss: 5.1519 - val_accuracy: 0.0056\n",
      "Epoch 2/10\n",
      "2100/2100 [==============================] - 611s 291ms/step - loss: 5.0697 - accuracy: 0.0118 - val_loss: 4.9562 - val_accuracy: 0.0063\n",
      "Epoch 3/10\n",
      "2100/2100 [==============================] - 611s 291ms/step - loss: 4.8884 - accuracy: 0.0179 - val_loss: 4.7779 - val_accuracy: 0.0080\n",
      "Epoch 4/10\n",
      "2100/2100 [==============================] - 611s 291ms/step - loss: 4.7180 - accuracy: 0.0276 - val_loss: 4.5010 - val_accuracy: 0.0185\n",
      "Epoch 5/10\n",
      "2100/2100 [==============================] - 611s 291ms/step - loss: 4.3220 - accuracy: 0.0614 - val_loss: 3.8619 - val_accuracy: 0.0837\n",
      "Epoch 6/10\n",
      "1953/2100 [==========================>...] - ETA: 39s - loss: 3.6484 - accuracy: 0.1456"
     ]
    }
   ],
   "source": [
    "\n",
    "#history = model.fit_generator(\n",
    "history = model.fit(\n",
    "    Gen_Train,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    workers=16,\n",
    "    validation_data = Gen_Val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_Val_Test(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('../Data/Model/ResNet-20200301_batch_norm2.wgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    plt.imshow(display_grid, aspect='auto', cmap='viridis')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = Model(inputs = model.input, outputs = successive_outputs)\n",
    "# Let's prepare a random input image from the training set.\n",
    "#horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "#human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "#img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = cv2.imread('../Data/Train/Train_1.jpg', cv2.IMREAD_GRAYSCALE)  # this is a PIL image\n",
    "x = img.reshape(1,90,160,1) # Numpy array with shape (150, 150, 3)\n",
    "x = x / 255\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "# Now let's display our representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    print(feature_map.shape)\n",
    "    if len(feature_map.shape) > 0 :\n",
    "        print('nice')\n",
    "        # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "        n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "        # The feature map has shape (1, size, size, n_features)\n",
    "        size = feature_map.shape[1]\n",
    "        # We will tile our images in this matrix\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "        for i in range(n_features):\n",
    "            # Postprocess the feature to make it visually palatable\n",
    "            #x = feature_map[0, :, :, i]\n",
    "            x = feature_map[0, i]\n",
    "            x -= x.mean()\n",
    "            x /= x.std()\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "            # We'll tile each filter into this big horizontal grid\n",
    "        display_grid[:, i * size : (i + 1) * size] = x\n",
    "        # Display the grid\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "     plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model23' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6a7647bafd69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# intermediate representations for all layers in the previous model after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# the first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msuccessive_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel23\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#visualization_model = Model(img_input, successive_outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvisualization_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel23\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuccessive_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model23' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model23.layers[1:]]\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = Model(inputs = model23.input, outputs = successive_outputs)\n",
    "# Let's prepare a random input image from the training set.\n",
    "#horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "#human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "#img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = cv2.imread('/home/beltain/Data/fmnist/img_0.jpg', cv2.IMREAD_GRAYSCALE)  # this is a PIL image\n",
    "x = img.reshape(1,28,28,1) # Numpy array with shape (150, 150, 3)\n",
    "x = x / 255\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model23.layers]\n",
    "\n",
    "# Now let's display our representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    print(feature_map.shape)\n",
    "    if len(feature_map.shape) > 0 :\n",
    "        print('nice')\n",
    "        # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "        n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "        # The feature map has shape (1, size, size, n_features)\n",
    "        size = feature_map.shape[1]\n",
    "        # We will tile our images in this matrix\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "        for i in range(n_features):\n",
    "            # Postprocess the feature to make it visually palatable\n",
    "            #x = feature_map[0, :, :, i]\n",
    "            x = feature_map[0, i]\n",
    "            x -= x.mean()\n",
    "            x /= x.std()\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "            # We'll tile each filter into this big horizontal grid\n",
    "        display_grid[:, i * size : (i + 1) * size] = x\n",
    "        # Display the grid\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
