{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenetv2 for Testing - Take 2\n",
    "We are going to use a mobilnet to train a model and see how it does\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import DataGenerator, Plot_Val_Test\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 #for our model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in CSV Data and creating Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train is: 160562 x 7 \n",
      "Shape of Val is 40278 x 7\n"
     ]
    }
   ],
   "source": [
    "CSV = pd.read_csv('../Data/train_extra.csv')\n",
    "Counts_Per_Class = 750\n",
    "# Balancing the Classes\n",
    "\n",
    "\n",
    "# spling into train and validate\n",
    "Idx_Split = np.random.uniform(size = CSV.shape[0])\n",
    "CSV_Val = CSV[Idx_Split <= 0.2].copy().reset_index()\n",
    "CSV_Train = CSV[Idx_Split > 0.2].copy().reset_index()\n",
    "\n",
    "#rebalcing CSV_train\n",
    "if False:\n",
    "    CSV_Train['index'] = CSV_Train.index\n",
    "    Re_Index = CSV_Train.groupby('grapheme_root').index.apply(lambda x: x.sample(n=1000, replace=True))\n",
    "    CSV_Train = CSV_Train.loc[Re_Index.index.droplevel(),:]\n",
    "    CSV_Train.reset_index(inplace = True)\n",
    "    CSV_Train.drop(columns = ['index','level_0'], inplace = True)\n",
    "\n",
    "print(\"Shape of Train is: {} x {} \\nShape of Val is {} x {}\".format(CSV_Train.shape[0], \n",
    "                                                                    CSV_Train.shape[1],\n",
    "                                                                    CSV_Val.shape[0], \n",
    "                                                                    CSV_Val.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>Image_Dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "      <td>../Data/Train/Train_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "      <td>../Data/Train/Train_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "      <td>../Data/Train/Train_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>র্টি</td>\n",
       "      <td>../Data/Train/Train_3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "      <td>../Data/Train/Train_4.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme  \\\n",
       "0  Train_0             15                9                    5   ক্ট্রো   \n",
       "1  Train_1            159                0                    0        হ   \n",
       "2  Train_2             22                3                    5     খ্রী   \n",
       "3  Train_3             53                2                    2     র্টি   \n",
       "4  Train_4             71                9                    5     থ্রো   \n",
       "\n",
       "                   Image_Dir  \n",
       "0  ../Data/Train/Train_0.jpg  \n",
       "1  ../Data/Train/Train_1.jpg  \n",
       "2  ../Data/Train/Train_2.jpg  \n",
       "3  ../Data/Train/Train_3.jpg  \n",
       "4  ../Data/Train/Train_4.jpg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encodings is 168\n",
      "Number of Encodings is 168\n",
      "Target has 168 values,\n",
      "number of batches per epoch is 1050 x 251\n"
     ]
    }
   ],
   "source": [
    "Gen_Train = DataGenerator(\n",
    "                csv_file = CSV_Train,\n",
    "                 y_var = 'grapheme_root', # 'consonant_diacritic',\n",
    "                 to_fit=True,\n",
    "                 batch_size=120, #160,\n",
    "                 dim = (90,160),\n",
    "                 channels = 1,\n",
    "                 vertical_flip = 0,\n",
    "                 horizontal_flip = 0,\n",
    "                 rotate = (0.2,180), #PRob, max roatioan\n",
    "                 shear = (0.2,0.9), # prob, max_shear\n",
    "                 shuffle=True,\n",
    "                sample_classes = 750,\n",
    "                save_model_path = '../Data/Best_Model_eva.hdf5')\n",
    "\n",
    "Gen_Val = DataGenerator(\n",
    "                csv_file = CSV_Val,\n",
    "                 y_var = 'grapheme_root', # 'consonant_diacritic',\n",
    "                 to_fit=True,\n",
    "                 batch_size=160,\n",
    "                 dim = (90,160),\n",
    "                 channels = 1,\n",
    "                 vertical_flip = 0,\n",
    "                 horizontal_flip = 0,\n",
    "                 rotate = (0,0), #Prob, max roatioan\n",
    "                 shear = (0,0), # prob, max_shear\n",
    "                 shuffle=True,\n",
    "                 sample_classes = 0)\n",
    "\n",
    "\n",
    "print(\"Target has {} values,\\nnumber of batches per epoch is {} x {}\".format(Gen_Train.y.shape[1],\n",
    "                                                                                 len(Gen_Train),\n",
    "                                                                                 len(Gen_Val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MNV2(input_shape: tuple,\n",
    "               alpha: int = 1,\n",
    "               weights: list = None,\n",
    "               dropout_per: float = 0.2,\n",
    "               target_size: int = 168,\n",
    "              learning_rate: float = 0.0002):\n",
    "    mobilenetV2 = MobileNetV2(\n",
    "                input_shape = input_shape,\n",
    "                alpha = alpha,\n",
    "                weights=weights,\n",
    "                include_top=False)\n",
    "\n",
    "    #Making all layers in MobilenetV2 trainable!\n",
    "    for layer in mobilenetV2.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(mobilenetV2)\n",
    "    model.add(layers.GlobalMaxPooling2D())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1024, activation = 'relu'))\n",
    "    model.add(layers.Dropout(dropout_per))\n",
    "    model.add(layers.Dense(512, activation = 'relu'))\n",
    "    model.add(layers.Dropout(dropout_per))\n",
    "    model.add(layers.Dense(256, activation = 'relu'))\n",
    "    model.add(layers.Dropout(dropout_per))\n",
    "    model.add(layers.Dense(target_size, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizers.Adam(lr=learning_rate),\n",
    "        #optimizer = optimizers.RMSprop(learning_rate = learning_rate )    ,\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_minst_big(input_shape,\n",
    "                dropout_per,\n",
    "               target_size,\n",
    "               learning_rate = 0.0002):\n",
    "    \n",
    "    mnst_model = Sequential([\n",
    "    # This is the first convolution\n",
    "    layers.Conv2D(128, (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # The forth Conco\n",
    "    layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(dropout_per),\n",
    "        \n",
    "#    layers.Dense(256, activation='relu'),\n",
    "#    layers.Dropout(dropout_per),\n",
    "        \n",
    "    layers.Dense(target_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    mnst_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizers.Adam(lr=learning_rate),\n",
    "        #optimizer = optimizers.RMSprop(learning_rate = learning_rate )    ,\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return mnst_model\n",
    "    \n",
    "def build_minst(input_shape,\n",
    "                dropout_per,\n",
    "               target_size,\n",
    "               learning_rate = 0.0002):\n",
    "    \n",
    "    mnst_model = Sequential([\n",
    "    # This is the first convolution\n",
    "    layers.Conv2D(16, (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.Conv2D(8, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(dropout_per),\n",
    "        \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(dropout_per),\n",
    "        \n",
    "    layers.Dense(target_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    mnst_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizers.Adam(lr=learning_rate),\n",
    "        #optimizer = optimizers.RMSprop(learning_rate = learning_rate )    ,\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return mnst_model\n",
    "\n",
    "\n",
    "def build_minst2(input_shape,\n",
    "                dropout_per,\n",
    "               target_size,\n",
    "               learning_rate = 0.0002):\n",
    "    \n",
    "    mnst_model = Sequential([\n",
    "    # This is the first convolution\n",
    "    layers.Conv2D(16, (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(dropout_per),\n",
    "        \n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(dropout_per),\n",
    "        \n",
    "    layers.Dense(target_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    mnst_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizers.Adam(lr=learning_rate),\n",
    "        #optimizer = optimizers.RMSprop(learning_rate = learning_rate )    ,\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return mnst_model\n",
    "\n",
    "def build_springer(input_shape,\n",
    "                dropout_per,\n",
    "               target_size,\n",
    "               learning_rate = 0.0002,\n",
    "                return_features = True):\n",
    "    \"\"\"Model based on this paper for character recognition\n",
    "    https://link.springer.com/article/10.1007/s11036-019-01243-5\n",
    "    \"\"\"\n",
    "    mnst_model = Sequential([\n",
    "    # This is the first convolution\n",
    "    layers.Conv2D(128, (5,5), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    layers.Conv2D(128, (5,5), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    layers.Conv2D(256, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # The Forth Layer\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    # drop out layer\n",
    "    layers.Dropout(dropout_per),\n",
    "    layers.Dense(target_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    mnst_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizers.Adam(lr=learning_rate),\n",
    "        #optimizer = optimizers.RMSprop(learning_rate = learning_rate )    ,\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    return mnst_model\n",
    "def build_mini_minst(input_shape,\n",
    "                dropout_per,\n",
    "               target_size,\n",
    "               learning_rate = 0.0002):\n",
    "    \n",
    "    mnst_model = Sequential([\n",
    "    # This is the first convolution\n",
    "    layers.Conv2D(20, (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    layers.Conv2D(10, (3,3), activation = 'relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # Third layer\n",
    "    layers.Conv2D(5, (3,3), activation = 'relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    # drop out layer\n",
    "    layers.Dropout(dropout_per),\n",
    "    layers.Dense(target_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    mnst_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizers.Adam(lr=learning_rate),\n",
    "        #optimizer = optimizers.RMSprop(learning_rate = learning_rate )    ,\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return mnst_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_90 (Model)  (None, 3, 5, 1280)        2257408   \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              1311744   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 168)               43176     \n",
      "=================================================================\n",
      "Total params: 4,268,456\n",
      "Trainable params: 4,234,344\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_MNV2(input_shape = (90,160,1), dropout_per = 0.4, target_size = 168, learning_rate =  0.00001)\n",
    "model.load_weights('../Data/Model/MobileNet-20200215.wgt')\n",
    "#model = build_minst(input_shape = (90,160,1), dropout_per = 0.5, target_size = 168, learning_rate = 0.001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1050 steps, validate for 251 steps\n",
      "Epoch 1/5\n",
      "1050/1050 [==============================] - 200s 191ms/step - loss: 0.5505 - accuracy: 0.8777 - val_loss: 0.3925 - val_accuracy: 0.9053\n",
      "Epoch 2/5\n",
      " 293/1050 [=======>......................] - ETA: 2:13 - loss: 0.5324 - accuracy: 0.8797"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-25741f7cd902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     validation_data = Gen_Val)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \"\"\"\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \"\"\"\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#history = model.fit_generator(\n",
    "history = model.fit(\n",
    "    Gen_Train,\n",
    "    #epochs=30,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    workers=16,\n",
    "    validation_data = Gen_Val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Plot_Val_Test(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('../Data/Model/MobileNet-20200215.wgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    plt.imshow(display_grid, aspect='auto', cmap='viridis')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = Model(inputs = model.input, outputs = successive_outputs)\n",
    "# Let's prepare a random input image from the training set.\n",
    "#horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "#human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "#img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = cv2.imread('../Data/Train/Train_1.jpg', cv2.IMREAD_GRAYSCALE)  # this is a PIL image\n",
    "x = img.reshape(1,90,160,1) # Numpy array with shape (150, 150, 3)\n",
    "x = x / 255\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "# Now let's display our representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    print(feature_map.shape)\n",
    "    if len(feature_map.shape) > 0 :\n",
    "        print('nice')\n",
    "        # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "        n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "        # The feature map has shape (1, size, size, n_features)\n",
    "        size = feature_map.shape[1]\n",
    "        # We will tile our images in this matrix\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "        for i in range(n_features):\n",
    "            # Postprocess the feature to make it visually palatable\n",
    "            #x = feature_map[0, :, :, i]\n",
    "            x = feature_map[0, i]\n",
    "            x -= x.mean()\n",
    "            x /= x.std()\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "            # We'll tile each filter into this big horizontal grid\n",
    "        display_grid[:, i * size : (i + 1) * size] = x\n",
    "        # Display the grid\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "     plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model23' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6a7647bafd69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# intermediate representations for all layers in the previous model after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# the first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msuccessive_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel23\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#visualization_model = Model(img_input, successive_outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvisualization_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel23\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuccessive_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model23' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model23.layers[1:]]\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = Model(inputs = model23.input, outputs = successive_outputs)\n",
    "# Let's prepare a random input image from the training set.\n",
    "#horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "#human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "#img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = cv2.imread('/home/beltain/Data/fmnist/img_0.jpg', cv2.IMREAD_GRAYSCALE)  # this is a PIL image\n",
    "x = img.reshape(1,28,28,1) # Numpy array with shape (150, 150, 3)\n",
    "x = x / 255\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model23.layers]\n",
    "\n",
    "# Now let's display our representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    print(feature_map.shape)\n",
    "    if len(feature_map.shape) > 0 :\n",
    "        print('nice')\n",
    "        # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "        n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "        # The feature map has shape (1, size, size, n_features)\n",
    "        size = feature_map.shape[1]\n",
    "        # We will tile our images in this matrix\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "        for i in range(n_features):\n",
    "            # Postprocess the feature to make it visually palatable\n",
    "            #x = feature_map[0, :, :, i]\n",
    "            x = feature_map[0, i]\n",
    "            x -= x.mean()\n",
    "            x /= x.std()\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "            # We'll tile each filter into this big horizontal grid\n",
    "        display_grid[:, i * size : (i + 1) * size] = x\n",
    "        # Display the grid\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
