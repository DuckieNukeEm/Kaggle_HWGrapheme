{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenetv2 for Testing\n",
    "We are going to use a mobilnet to train a model and see how it does\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 #for our model\n",
    "from tensorflow.keras.utils import Sequence, to_categorical # For our own data generator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n",
    "import cv2 # For image processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/keras-data-generators-and-how-to-use-them-b69129ed779c\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Generates data for Keras\n",
    "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 csv_file, # file that has the images on it, as well as the image types\n",
    "                 y_var = 'grapheme_root', #'grapheme_root','vowel_diacritic','consonant_diacritic' \n",
    "                 to_fit=True,\n",
    "                 batch_size=32,\n",
    "                 dim = (90,160),\n",
    "                 channels = 1,\n",
    "                 vertical_flip = 0,\n",
    "                 horizontal_flip = 0,\n",
    "                 rotate = (0,0), #PRob, max roatioan\n",
    "                 shear = (0,0), # prob, max_shear\n",
    "                 shuffle=True):\n",
    "        \"\"\"Initialization\n",
    "        :param csv_File #CSV file that has the path to the stores on it\n",
    "        :param y_var: a list of 'root','voewl','consonant'\n",
    "        :param to_fit: Provive the dependent variable as well\n",
    "        :param batch_size: The size of each batch to deliver\n",
    "        :param dim: dimensions of the photos to use\n",
    "        :param channels: The number of channals of the photo - 1 is bw, 3 is color, any other is customer\n",
    "        :param vertical_flip: (dbl) The percent chance to flip a photo along a vertical axis\n",
    "        :param horizontal_flip: (dbl) The percent chance to flip a photo along a horiszontal axis\n",
    "        :param rotate: (tuple - (prob, degree)) A two unit tuple, first is the % chance of rotate, the next is the amount of rotation\n",
    "        :param shear: (tuple - (prob, amt)) A two unit tuple, first is the % chance of shear, the next is the amount of shear\n",
    "        :param shuffle: True to shuffle label indexes after every epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        #Getting Index that we will use to sort\n",
    "        \n",
    "        self.Idx_List = np.arange(csv_file.shape[0])\n",
    "        \n",
    "        # Loading y_Vars\n",
    "        if isinstance(y_var, list):\n",
    "            y_var = y_var[0]\n",
    "            \n",
    "        self.y_var = csv_file[y_var].values\n",
    "       \n",
    "        #TODO Move this to the aabove if statement, removing the need for y_var, and y\n",
    "        self.y_dim = 0\n",
    "        self.hot_encode_y()\n",
    "        #self.y = self.y_var\n",
    "        # getting Images Location\n",
    "        self.Imgs = csv_file.Image_Dir.values\n",
    "        \n",
    "        #Setting other vars\n",
    "        self.batch_size = batch_size\n",
    "        self.fit = to_fit\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.channels = channels\n",
    "        if self.channels == 1:\n",
    "            self.read_mode = cv2.IMREAD_GRAYSCALE\n",
    "        elif self.channels == 3:\n",
    "            self.read_mode = cv2.IMREAD_COLOR\n",
    "        else:\n",
    "            self.read_mode = cv2.IMREAD_UNCHANGED\n",
    "        \n",
    "        assert 0 <= vertical_flip <=1, \"vertical_flip = {}, which is not between 0 or 1\".format(vertical_flip)\n",
    "        self.v_flip = vertical_flip\n",
    "        \n",
    "        assert 0 <= horizontal_flip <=1, \"horizontal_flip = {}, which is not between 0 or 1\".format(horizontal_flip)\n",
    "        self.h_flip = horizontal_flip\n",
    "        \n",
    "        assert 0 <= rotate[0] <=1, \"first value of rotate = {}, which is not between 0 or 1\".format(rotate[0])\n",
    "        self.r_prob = rotate[0]\n",
    "        assert 0 <= rotate[1] < 360, \"second value of rotate = {}, which is not between 0 or 359\".format(rotate[1])\n",
    "        self.r_deg = rotate[1]\n",
    "        \n",
    "        assert 0 <= shear[0] <=1, \"first value of shear = {}, which is not between 0 or 1\".format(shear[0])\n",
    "        self.s_prob = shear[0]\n",
    "        \n",
    "        assert 0 <= shear[1] <=359, \"first value of shear = {}, which is not between 0 or 359\".format(shear[1])\n",
    "        self.s_fact = shear[1]\n",
    "        #self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\n",
    "        :return: number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.Idx_List) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data\n",
    "        :param index: index of the batch\n",
    "        :return: X and y when fitting. X only when predicting\n",
    "        \"\"\"\n",
    "        # Generate indexes of the batch\n",
    "        Batch_Idx = self.Idx_List[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "\n",
    "        # Generate data\n",
    "        X = self._generate_X(Batch_Idx)\n",
    "\n",
    "        if self.fit:\n",
    "            y = self._generate_y(Batch_Idx)\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "    #TODO at a def __iter__ and __next__ methodology so we can loop through it!\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        #self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.Idx_List)\n",
    "\n",
    "    def _generate_X(self, Batch_Idx):\n",
    "        \"\"\"Generates data containing batch_size images\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch of images\n",
    "        \"\"\"\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.channels))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(Batch_Idx):\n",
    "            # Store sample\n",
    "            X[i,] = self._load_image(self.Imgs[ID])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _generate_y_bck(self, Batch_Idx):\n",
    "        \"\"\"Generates data containing batch_size masks\n",
    "        :param list_IDs_temp: list of label ids to load\n",
    "        :return: batch if masks\n",
    "        \"\"\"\n",
    "        y = np.empty((self.batch_size, *self.dim), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(Batch_Idx):\n",
    "            # Store sample\n",
    "            y[i,] = self._load_image(self.mask_path + self.labels[ID])\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def _generate_y(self, Batch_Idx):\n",
    "        y = self.y[Batch_Idx,:]\n",
    "        #y = self.y[Batch_Idx]\n",
    "        return y\n",
    "    \n",
    "    def hot_encode_y(self):\n",
    "        #self.y = pd.get_dummies(pd.Categorical(self.y_var)).values\n",
    "        self.y = to_categorical(self.y_var)\n",
    "        print(\"Number of Encodings is {}\".format(self.y.shape[1]))\n",
    "        self.y_dim = self.y.shape[1]\n",
    "        \n",
    "\n",
    "\n",
    "    def _load_image(self, image_path):\n",
    "        \n",
    "        img = cv2.imread(image_path,  self.read_mode) #load an image as grayscale\n",
    "        \n",
    "        if img.shape != self.dim:\n",
    "            img = cv2.resize(img, self.dim)\n",
    "        \n",
    "        img = self._flip_vertical(img)\n",
    "        \n",
    "        img = self._flip_horizontal(img)\n",
    "        \n",
    "        img = self._rotate(img)\n",
    "        \n",
    "        img = self._shear(img)\n",
    "        \n",
    "        img = img/255.0\n",
    "        if self.channels == 1:\n",
    "            #img = img.reshape(self.dim[0],self.dim[1],self.channels)\n",
    "            img = np.expand_dims(img,2)\n",
    "\n",
    "        return(img)\n",
    "        \n",
    "    def _flip_vertical(self, img):\n",
    "        \"\"\"Flips and image on a  vertica axis, as set by v_flip,\n",
    "        if set to zero, will never flip the image \"\"\"\n",
    "        if 1 - self.v_flip < np.random.uniform():\n",
    "            img = cv2.flip(img,1)\n",
    "            return(img)\n",
    "        else:\n",
    "            return(img)\n",
    "        \n",
    "    def _flip_horizontal(self, img):\n",
    "        \"\"\"Flips and image on a horizontal axis, as set by v_flip,\n",
    "        if set to zero, will never flip the image \"\"\"\n",
    "        if 1 - self.h_flip < np.random.uniform():\n",
    "            img = cv2.flip(img,0)\n",
    "            return(img)\n",
    "        else:\n",
    "            return(img)\n",
    "        \n",
    "    def _rotate(self, img):\n",
    "        if 1 - self.r_prob < np.random.uniform():\n",
    "            rows, cols = img.shape[0], img.shape[1]\n",
    "            #TODO allow a rotation both forward and backwards\n",
    "            Rot_M = cv2.getRotationMatrix2D((cols / 2, rows / 2), np.random.uniform(self.r_deg), 1)\n",
    "            img = cv2.warpAffine(img, Rot_M, (cols, rows))\n",
    "            return(img)\n",
    "        else:\n",
    "            return(img)\n",
    "        \n",
    "    def _shear(self, img):\n",
    "        \"\"\"\n",
    "        https://blog.paperspace.com/data-augmentation-for-object-detection-rotation-and-shearing/\n",
    "        \"\"\"\n",
    "        if 1 - self.s_prob < np.random.uniform():\n",
    "            #TODO allow a shear both forward and backwards\n",
    "            rows, cols = img.shape[0], img.shape[1]\n",
    "            \n",
    "            shear_factor = np.random.uniform(self.s_fact)\n",
    "            \n",
    "            M = np.array([[1, shear_factor, 0],[0,1,0]])\n",
    "            nW =  RR.shape[1] + abs(shear_factor*img.shape[0])\n",
    "            img = cv2.warpAffine(img, M, (int(nW), img.shape[0]))\n",
    "            img = cv2.resize(img, (cols,rows))\n",
    "            return(img)\n",
    "        else:\n",
    "            return(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encodings is 168\n"
     ]
    }
   ],
   "source": [
    "Gen_Train2 = DataGenerator(\n",
    "                csv_file = CSV_Train,\n",
    "                 y_var = 'root',\n",
    "                 to_fit=True,\n",
    "                 batch_size=2,\n",
    "                 dim = (90,160),\n",
    "                 channels = 1,\n",
    "                 vertical_flip = 0,\n",
    "                 horizontal_flip = 0,\n",
    "                 rotate = (.50,90), #PRob, max roatioan\n",
    "                 shear = (0,0), # prob, max_shear\n",
    "                 shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 160)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ZZ = Gen_Train2[1]\n",
    "plt.imshow(ZZ[0][1].reshape((90,160)))\n",
    "ZZ[0][1].shape\n",
    "#ZZ_1 = ZZ[0].reshape(ZZ[0].shape[0],ZZ[0].shape[1], ZZ[0].shape[2],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in CSV Data and creating Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train is: 136369 x 7 \n",
      "Shape of Val is 34268 x 7\n"
     ]
    }
   ],
   "source": [
    "CSV = pd.read_csv('../Data/train_extra.csv')\n",
    "\n",
    "#removing Test\n",
    "CSV = CSV[CSV.Train == 0]\n",
    "\n",
    "# spling into train and validate\n",
    "Idx_Split = np.random.uniform(size = CSV.shape[0])\n",
    "CSV_Val = CSV[Idx_Split <= 0.2]\n",
    "CSV_Train = CSV[Idx_Split > 0.2]\n",
    "\n",
    "print(\"Shape of Train is: {} x {} \\nShape of Val is {} x {}\".format(CSV_Train.shape[0], \n",
    "                                                                    CSV_Train.shape[1],\n",
    "                                                                    CSV_Val.shape[0], \n",
    "                                                                    CSV_Val.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "      <th>Train</th>\n",
       "      <th>Image_Dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>ক্ট্রো</td>\n",
       "      <td>0</td>\n",
       "      <td>../Data/Train/Train_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ</td>\n",
       "      <td>0</td>\n",
       "      <td>../Data/Train/Train_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>খ্রী</td>\n",
       "      <td>0</td>\n",
       "      <td>../Data/Train/Train_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রো</td>\n",
       "      <td>0</td>\n",
       "      <td>../Data/Train/Train_4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Train_7</td>\n",
       "      <td>139</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>ষী</td>\n",
       "      <td>0</td>\n",
       "      <td>../Data/Train/Train_7.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme  \\\n",
       "0  Train_0             15                9                    5   ক্ট্রো   \n",
       "1  Train_1            159                0                    0        হ   \n",
       "2  Train_2             22                3                    5     খ্রী   \n",
       "4  Train_4             71                9                    5     থ্রো   \n",
       "7  Train_7            139                3                    0       ষী   \n",
       "\n",
       "   Train                  Image_Dir  \n",
       "0      0  ../Data/Train/Train_0.jpg  \n",
       "1      0  ../Data/Train/Train_1.jpg  \n",
       "2      0  ../Data/Train/Train_2.jpg  \n",
       "4      0  ../Data/Train/Train_4.jpg  \n",
       "7      0  ../Data/Train/Train_7.jpg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CSV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encodings is 168\n",
      "Number of Encodings is 168\n",
      "Shape of target is 2 , number of batches per epoch is 1067 x 1064\n"
     ]
    }
   ],
   "source": [
    "Gen_Train = DataGenerator(\n",
    "                csv_file = CSV_Train,\n",
    "                 y_var = 'root',\n",
    "                 to_fit=True,\n",
    "                 batch_size=128,\n",
    "                 dim = (90,160),\n",
    "                 channels = 1,\n",
    "                 vertical_flip = 0,\n",
    "                 horizontal_flip = 0,\n",
    "                 rotate = (.50,90), #PRob, max roatioan\n",
    "                 shear = (0,0), # prob, max_shear\n",
    "                 shuffle=True)\n",
    "\n",
    "Gen_Val = DataGenerator(\n",
    "                csv_file = CSV_Val,\n",
    "                 y_var = 'root',\n",
    "                 to_fit=True,\n",
    "                 batch_size=32,\n",
    "                 dim = (90,160),\n",
    "                 channels = 1,\n",
    "                 vertical_flip = 0,\n",
    "                 horizontal_flip = 0,\n",
    "                 rotate = (0,0), #Prob, max roatioan\n",
    "                 shear = (0,0), # prob, max_shear\n",
    "                 shuffle=True)\n",
    "\n",
    "\n",
    "print(\"Shape of target is {} , number of batches per epoch is {} x {}\".format(len(np.unique(Gen_Train.y)),\n",
    "                                                                                 len(Gen_Train),\n",
    "                                                                                 len(Gen_Val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MNV2(input_shape,\n",
    "               alpha = 1,\n",
    "               weights = None,\n",
    "               dropout_per = 0.2,\n",
    "               target_size = 168,\n",
    "              learning_rate = 0.0002):\n",
    "    mobilenetV2 = MobileNetV2(\n",
    "        input_shape = input_shape,\n",
    "        alpha = alpha,\n",
    "        weights=weights,\n",
    "        include_top=False\n",
    "    )\n",
    "    #Making all layers in MobilenetV2 trainable!\n",
    "    for layer in mobilenetV2.layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(mobilenetV2)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dropout(dropout_per))\n",
    "    model.add(layers.Dense(target_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizers.Adam(lr=learning_rate),\n",
    "        #optimizer = optimizers.RMSprop(learning_rate = learning_rate )    ,\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "  \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_minst(input_shape,\n",
    "                dropout_per,\n",
    "               target_size,\n",
    "               learning_rate = 0.0002):\n",
    "    \n",
    "    mnst_model = Sequential([\n",
    "    # This is the first convolution\n",
    "    layers.Conv2D(16, (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    # drop out layer\n",
    "    layers.Dropout(dropout_per),\n",
    "    layers.Dense(target_size, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    mnst_model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizers.Adam(lr=learning_rate),\n",
    "        #optimizer = optimizers.RMSprop(learning_rate = learning_rate )    ,\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return mnst_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.engine.sequential.Sequential"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_MNV2(input_shape = (90,160,1))\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0130 13:06:05.271191 140040918865728 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068/1068 [==============================] - 844s 790ms/step - loss: 4.7391 - accuracy: 0.0265 - val_loss: 5.8342 - val_accuracy: 0.0282\n",
      "Epoch 2/10\n",
      "1068/1068 [==============================] - 844s 791ms/step - loss: 4.7051 - accuracy: 0.0274 - val_loss: 5.2735 - val_accuracy: 0.0278\n",
      "Epoch 3/10\n",
      "1068/1068 [==============================] - 843s 789ms/step - loss: 4.7014 - accuracy: 0.0279 - val_loss: 4.8950 - val_accuracy: 0.0278\n",
      "Epoch 4/10\n",
      "1068/1068 [==============================] - 844s 790ms/step - loss: 4.7009 - accuracy: 0.0280 - val_loss: 4.7566 - val_accuracy: 0.0281\n",
      "Epoch 5/10\n",
      "1068/1068 [==============================] - 843s 790ms/step - loss: 4.6985 - accuracy: 0.0278 - val_loss: 4.7139 - val_accuracy: 0.0282\n",
      "Epoch 6/10\n",
      "1068/1068 [==============================] - 845s 791ms/step - loss: 4.6975 - accuracy: 0.0276 - val_loss: 4.7982 - val_accuracy: 0.0280\n",
      "Epoch 7/10\n",
      "1068/1068 [==============================] - 843s 789ms/step - loss: 4.6981 - accuracy: 0.0276 - val_loss: 4.6934 - val_accuracy: 0.0265\n",
      "Epoch 8/10\n",
      "1068/1068 [==============================] - 844s 791ms/step - loss: 4.6987 - accuracy: 0.0276 - val_loss: 5.0000 - val_accuracy: 0.0282\n",
      "Epoch 9/10\n",
      "1068/1068 [==============================] - 845s 791ms/step - loss: 4.6993 - accuracy: 0.0280 - val_loss: 4.7002 - val_accuracy: 0.0274\n",
      "Epoch 10/10\n",
      "1068/1068 [==============================] - 843s 790ms/step - loss: 4.6967 - accuracy: 0.0277 - val_loss: 4.6936 - val_accuracy: 0.0293\n"
     ]
    }
   ],
   "source": [
    "model = build_MNV2(input_shape = (90,160,1), adam_learning_rate = 0.002)\n",
    "model = compile_model(model, 0.002)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    Gen_Train,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_data = Gen_Val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_45 (Conv2D)           (None, 88, 158, 16)       160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 44, 79, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 42, 77, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 21, 38, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 25536)             0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               3268736   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 168)               21672     \n",
      "=================================================================\n",
      "Total params: 3,295,208\n",
      "Trainable params: 3,295,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = build_minst(input_shape = (90,160,1), dropout_per = 0.2, target_size = 168)\n",
    "model2 = compile_model(model2, 0.2)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_45_input to have 4 dimensions, but got array with shape (128, 90, 160)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-a1f0befee4f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGen_Val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    977\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    978\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    980\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    981\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    251\u001b[0m   x, y, sample_weights = model._standardize_user_data(\n\u001b[1;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    254\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2435\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2436\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2437\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    562\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_45_input to have 4 dimensions, but got array with shape (128, 90, 160)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "history2 = model2.fit_generator(\n",
    "    Gen_Train,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    validation_data = Gen_Val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-07605f05b808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mPlot_Val_Test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history2' is not defined"
     ]
    }
   ],
   "source": [
    "def Plot_Val_Test(Model_hist):\n",
    "    acc = Model_hist.history['accuracy']\n",
    "    val_acc = Model_hist.history['val_accuracy']\n",
    "    loss = Model_hist.history['loss']\n",
    "    val_loss = Model_hist.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend(loc=0)\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "\n",
    "Plot_Val_Test(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    plt.imshow(display_grid, aspect='auto', cmap='viridis')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = Model(inputs = model.input, outputs = successive_outputs)\n",
    "# Let's prepare a random input image from the training set.\n",
    "#horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "#human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "#img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = cv2.imread('../Data/Train/Train_1.jpg', cv2.IMREAD_GRAYSCALE)  # this is a PIL image\n",
    "x = img.reshape(1,90,160,1) # Numpy array with shape (150, 150, 3)\n",
    "x = x / 255\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "# Now let's display our representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    print(feature_map.shape)\n",
    "    if len(feature_map.shape) > 0 :\n",
    "        print('nice')\n",
    "        # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "        n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "        # The feature map has shape (1, size, size, n_features)\n",
    "        size = feature_map.shape[1]\n",
    "        # We will tile our images in this matrix\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "        for i in range(n_features):\n",
    "            # Postprocess the feature to make it visually palatable\n",
    "            #x = feature_map[0, :, :, i]\n",
    "            x = feature_map[0, i]\n",
    "            x -= x.mean()\n",
    "            x /= x.std()\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "            # We'll tile each filter into this big horizontal grid\n",
    "        display_grid[:, i * size : (i + 1) * size] = x\n",
    "        # Display the grid\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "     plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "mnist = fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TT = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/home/beltain/Data/fmnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(training_images,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LIST = [None] * 10000\n",
    "for i in range(10000):\n",
    "    SAVE_DIR = DIR + '/img_Test' + str(i) + '.jpg'\n",
    "    cv2.imwrite(SAVE_DIR, TT[1][0][i])\n",
    "    TEST_LIST[i] = SAVE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df  = pd.DataFrame(list(zip(SAVE_LIST, TT[0][1])), columns = ['Image_Dir','y'] )\n",
    "train_df['Test'] = 0\n",
    "test_df = pd.DataFrame(list(zip(TEST_LIST, TT[1][1])), columns = ['Image_Dir','y'] )\n",
    "test_df['Test'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Dir</th>\n",
       "      <th>y</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/home/beltain/Data/fmnist/img_0.jpg</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>/home/beltain/Data/fmnist/img_1.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>/home/beltain/Data/fmnist/img_2.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>/home/beltain/Data/fmnist/img_3.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>/home/beltain/Data/fmnist/img_4.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Image_Dir  y  Test\n",
       "0  /home/beltain/Data/fmnist/img_0.jpg  9     0\n",
       "1  /home/beltain/Data/fmnist/img_1.jpg  0     0\n",
       "2  /home/beltain/Data/fmnist/img_2.jpg  0     0\n",
       "3  /home/beltain/Data/fmnist/img_3.jpg  3     0\n",
       "4  /home/beltain/Data/fmnist/img_4.jpg  0     0"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_df = pd.concat([train_df, test_df])\n",
    "FINAL_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_df.to_csv('/home/beltain/Data/fmnist.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_df = pd.read_csv('/home/beltain/Data/fmnist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Encodings is 10\n",
      "Number of Encodings is 10\n"
     ]
    }
   ],
   "source": [
    "Gen_Train_fm = DataGenerator(\n",
    "                csv_file = FINAL_df[FINAL_df.Test == 0],\n",
    "                 y_var = 'y',\n",
    "                 to_fit=True,\n",
    "                 batch_size=128,\n",
    "                 dim = (28,28),\n",
    "                 channels = 1,\n",
    "                 vertical_flip = 0.5,\n",
    "                 horizontal_flip = 0.5,\n",
    "                 rotate = (.10,90), #PRob, max roatioan\n",
    "                 shear = (0,0), # prob, max_shear\n",
    "                 shuffle=True)\n",
    "\n",
    "Gen_Val_fm = DataGenerator(\n",
    "                csv_file = FINAL_df[FINAL_df.Test == 1],\n",
    "                 y_var = 'y',\n",
    "                 to_fit=True,\n",
    "                 batch_size=32,\n",
    "                 dim = (28,28),\n",
    "                 channels = 1,\n",
    "                 vertical_flip = 0,\n",
    "                 horizontal_flip = 0,\n",
    "                 rotate = (0,0), #Prob, max roatioan\n",
    "                 shear = (0,0), # prob, max_shear\n",
    "                 shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 3, 3, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 32,906\n",
      "Trainable params: 32,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model23 = build_minst(input_shape = (28,28,1), dropout_per =0.0, target_size = 10)\n",
    "\n",
    "model23.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f87552ac400>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Gen_Train_fm[0][0][1].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.5945 - accuracy: 0.7823 - val_loss: 0.5461 - val_accuracy: 0.8018\n",
      "Epoch 2/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.5789 - accuracy: 0.7872 - val_loss: 0.5488 - val_accuracy: 0.7967\n",
      "Epoch 3/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.5679 - accuracy: 0.7923 - val_loss: 0.5258 - val_accuracy: 0.8103\n",
      "Epoch 4/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.5595 - accuracy: 0.7951 - val_loss: 0.5396 - val_accuracy: 0.8016\n",
      "Epoch 5/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.5455 - accuracy: 0.7999 - val_loss: 0.5228 - val_accuracy: 0.8060\n",
      "Epoch 6/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.5402 - accuracy: 0.8031 - val_loss: 0.5076 - val_accuracy: 0.8163\n",
      "Epoch 7/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.5335 - accuracy: 0.8029 - val_loss: 0.5014 - val_accuracy: 0.8220\n",
      "Epoch 8/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.5225 - accuracy: 0.8093 - val_loss: 0.4906 - val_accuracy: 0.8218\n",
      "Epoch 9/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.5182 - accuracy: 0.8104 - val_loss: 0.4922 - val_accuracy: 0.8204\n",
      "Epoch 10/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.5113 - accuracy: 0.8131 - val_loss: 0.4742 - val_accuracy: 0.8297\n",
      "Epoch 11/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.5041 - accuracy: 0.8144 - val_loss: 0.4865 - val_accuracy: 0.8239\n",
      "Epoch 12/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4967 - accuracy: 0.8171 - val_loss: 0.4652 - val_accuracy: 0.8308\n",
      "Epoch 13/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4945 - accuracy: 0.8205 - val_loss: 0.4652 - val_accuracy: 0.8331\n",
      "Epoch 14/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4916 - accuracy: 0.8212 - val_loss: 0.4657 - val_accuracy: 0.8331\n",
      "Epoch 15/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4853 - accuracy: 0.8219 - val_loss: 0.4646 - val_accuracy: 0.8373\n",
      "Epoch 16/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4854 - accuracy: 0.8239 - val_loss: 0.4643 - val_accuracy: 0.8284\n",
      "Epoch 17/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4820 - accuracy: 0.8247 - val_loss: 0.4552 - val_accuracy: 0.8346\n",
      "Epoch 18/90\n",
      "468/468 [==============================] - 9s 19ms/step - loss: 0.4740 - accuracy: 0.8280 - val_loss: 0.4485 - val_accuracy: 0.8407\n",
      "Epoch 19/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4720 - accuracy: 0.8280 - val_loss: 0.4510 - val_accuracy: 0.8375\n",
      "Epoch 20/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4723 - accuracy: 0.8279 - val_loss: 0.4436 - val_accuracy: 0.8410\n",
      "Epoch 21/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4653 - accuracy: 0.8303 - val_loss: 0.4385 - val_accuracy: 0.8413\n",
      "Epoch 22/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4654 - accuracy: 0.8319 - val_loss: 0.4405 - val_accuracy: 0.8433\n",
      "Epoch 23/90\n",
      "468/468 [==============================] - 9s 19ms/step - loss: 0.4625 - accuracy: 0.8312 - val_loss: 0.4571 - val_accuracy: 0.8404\n",
      "Epoch 24/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4566 - accuracy: 0.8345 - val_loss: 0.4391 - val_accuracy: 0.8446\n",
      "Epoch 25/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4559 - accuracy: 0.8336 - val_loss: 0.4511 - val_accuracy: 0.8302\n",
      "Epoch 26/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4467 - accuracy: 0.8370 - val_loss: 0.4368 - val_accuracy: 0.8414\n",
      "Epoch 27/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4544 - accuracy: 0.8343 - val_loss: 0.4507 - val_accuracy: 0.8393\n",
      "Epoch 28/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4456 - accuracy: 0.8374 - val_loss: 0.4238 - val_accuracy: 0.8455\n",
      "Epoch 29/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4431 - accuracy: 0.8388 - val_loss: 0.4322 - val_accuracy: 0.8484\n",
      "Epoch 30/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4420 - accuracy: 0.8387 - val_loss: 0.4387 - val_accuracy: 0.8414\n",
      "Epoch 31/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4400 - accuracy: 0.8418 - val_loss: 0.4223 - val_accuracy: 0.8486\n",
      "Epoch 32/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4411 - accuracy: 0.8408 - val_loss: 0.4243 - val_accuracy: 0.8473\n",
      "Epoch 33/90\n",
      "468/468 [==============================] - 9s 19ms/step - loss: 0.4325 - accuracy: 0.8428 - val_loss: 0.4352 - val_accuracy: 0.8396\n",
      "Epoch 34/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4360 - accuracy: 0.8423 - val_loss: 0.4348 - val_accuracy: 0.8457\n",
      "Epoch 35/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4295 - accuracy: 0.8435 - val_loss: 0.4204 - val_accuracy: 0.8497\n",
      "Epoch 36/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4328 - accuracy: 0.8415 - val_loss: 0.4216 - val_accuracy: 0.8500\n",
      "Epoch 37/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4257 - accuracy: 0.8455 - val_loss: 0.4240 - val_accuracy: 0.8496\n",
      "Epoch 38/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4236 - accuracy: 0.8460 - val_loss: 0.4198 - val_accuracy: 0.8485\n",
      "Epoch 39/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4247 - accuracy: 0.8461 - val_loss: 0.4172 - val_accuracy: 0.8494\n",
      "Epoch 40/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4189 - accuracy: 0.8468 - val_loss: 0.4110 - val_accuracy: 0.8525\n",
      "Epoch 41/90\n",
      "468/468 [==============================] - 8s 18ms/step - loss: 0.4203 - accuracy: 0.8474 - val_loss: 0.4110 - val_accuracy: 0.8512\n",
      "Epoch 42/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4192 - accuracy: 0.8468 - val_loss: 0.4157 - val_accuracy: 0.8508\n",
      "Epoch 43/90\n",
      "468/468 [==============================] - 9s 18ms/step - loss: 0.4152 - accuracy: 0.8502 - val_loss: 0.4174 - val_accuracy: 0.8513\n",
      "Epoch 44/90\n",
      " 69/468 [===>..........................] - ETA: 5s - loss: 0.4241 - accuracy: 0.8427"
     ]
    }
   ],
   "source": [
    "history2 = model23.fit_generator(\n",
    "    Gen_Train_fm,\n",
    "    epochs=90,\n",
    "    verbose=1,\n",
    "     validation_data = Gen_Val_fm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13, 13, 16)\n",
      "nice\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 13 is out of bounds for axis 1 with size 13",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-6a7647bafd69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Postprocess the feature to make it visually palatable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m#x = feature_map[0, :, :, i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 13 is out of bounds for axis 1 with size 13"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model23.layers[1:]]\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = Model(inputs = model23.input, outputs = successive_outputs)\n",
    "# Let's prepare a random input image from the training set.\n",
    "#horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
    "#human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
    "#img_path = random.choice(horse_img_files + human_img_files)\n",
    "\n",
    "img = cv2.imread('/home/beltain/Data/fmnist/img_0.jpg', cv2.IMREAD_GRAYSCALE)  # this is a PIL image\n",
    "x = img.reshape(1,28,28,1) # Numpy array with shape (150, 150, 3)\n",
    "x = x / 255\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model23.layers]\n",
    "\n",
    "# Now let's display our representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "    print(feature_map.shape)\n",
    "    if len(feature_map.shape) > 0 :\n",
    "        print('nice')\n",
    "        # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "        n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "        # The feature map has shape (1, size, size, n_features)\n",
    "        size = feature_map.shape[1]\n",
    "        # We will tile our images in this matrix\n",
    "        display_grid = np.zeros((size, size * n_features))\n",
    "        for i in range(n_features):\n",
    "            # Postprocess the feature to make it visually palatable\n",
    "            #x = feature_map[0, :, :, i]\n",
    "            x = feature_map[0, i]\n",
    "            x -= x.mean()\n",
    "            x /= x.std()\n",
    "            x *= 64\n",
    "            x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "            # We'll tile each filter into this big horizontal grid\n",
    "        display_grid[:, i * size : (i + 1) * size] = x\n",
    "        # Display the grid\n",
    "        scale = 20. / n_features\n",
    "        plt.figure(figsize=(scale * n_features, scale))\n",
    "        plt.title(layer_name)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
